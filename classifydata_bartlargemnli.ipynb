{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gold labels and pairIDs\n",
    "gold_labels = pd.read_csv('subset_en.csv', sep='\\t')\n",
    "classified_data = pd.DataFrame(columns=['gold_label', 'English', 'German', 'Spanish', 'Swahili', 'Urdu', 'pairID'])\n",
    "classified_data['gold_label'] = gold_labels['gold_label']\n",
    "classified_data['pairID'] = gold_labels['pairID']\n",
    "\n",
    "# Classify English\n",
    "labels = ['entailment', 'contradiction', 'neutral']\n",
    "data = gold_labels\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "  #Get needed info\n",
    "  sentence1 = row['sentence1']\n",
    "  sentence2 = row['sentence2']\n",
    "\n",
    "  # Classify\n",
    "  encoded_input = tokenizer.encode_plus(\n",
    "    sentence1,\n",
    "    sentence2,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=200,\n",
    "    return_tensors=\"pt\")\n",
    "  raw_output = model(**encoded_input)[0]\n",
    "  predicted_label = raw_output.argmax().item()\n",
    "\n",
    "  # Add label to classidied_data\n",
    "  classified_data.at[index, 'English'] = labels[predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify German to English\n",
    "data = pd.read_csv('translated_subset_de.tsv', sep='\\t')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "  #Get needed info\n",
    "  sentence1 = row['translated_sentence1']\n",
    "  sentence2 = row['translated_sentence2']\n",
    "\n",
    "  # Classify\n",
    "  encoded_input = tokenizer.encode_plus(\n",
    "    sentence1,\n",
    "    sentence2,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=200,\n",
    "    return_tensors=\"pt\")\n",
    "  raw_output = model(**encoded_input)[0]\n",
    "  predicted_label = raw_output.argmax().item()\n",
    "\n",
    "  # Add label to classidied_data\n",
    "  classified_data.at[index, 'German'] = labels[predicted_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify Spanish to English\n",
    "data = pd.read_csv('translated_subset_es.tsv', sep='\\t')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "  #Get needed info\n",
    "  sentence1 = row['translated_sentence1']\n",
    "  sentence2 = row['translated_sentence2']\n",
    "\n",
    "  # Classify\n",
    "  encoded_input = tokenizer.encode_plus(\n",
    "    sentence1,\n",
    "    sentence2,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=200,\n",
    "    return_tensors=\"pt\")\n",
    "  raw_output = model(**encoded_input)[0]\n",
    "  predicted_label = raw_output.argmax().item()\n",
    "\n",
    "  # Add label to classidied_data\n",
    "  classified_data.at[index, 'Spanish'] = labels[predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify Swahili to English\n",
    "data = pd.read_csv('translated_subset_sw.tsv', sep='\\t')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "  #Get needed info\n",
    "  sentence1 = row['translated_sentence1']\n",
    "  sentence2 = row['translated_sentence2']\n",
    "\n",
    "  # Classify\n",
    "  encoded_input = tokenizer.encode_plus(\n",
    "    sentence1,\n",
    "    sentence2,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=200,\n",
    "    return_tensors=\"pt\")\n",
    "  raw_output = model(**encoded_input)[0]\n",
    "  predicted_label = raw_output.argmax().item()\n",
    "\n",
    "  # Add label to classidied_data\n",
    "  classified_data.at[index, 'Swahili'] = labels[predicted_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classify Urdu to English\n",
    "data = pd.read_csv('translated_subset_ur.tsv', sep='\\t')\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "  #Get needed info\n",
    "  sentence1 = row['translated_sentence1']\n",
    "  sentence2 = row['translated_sentence2']\n",
    "\n",
    "  # Classify\n",
    "  encoded_input = tokenizer.encode_plus(\n",
    "    sentence1,\n",
    "    sentence2,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=200,\n",
    "    return_tensors=\"pt\")\n",
    "  raw_output = model(**encoded_input)[0]\n",
    "  predicted_label = raw_output.argmax().item()\n",
    "\n",
    "  # Add label to classidied_data\n",
    "  classified_data.at[index, 'Urdu'] = labels[predicted_label]\n",
    "\n",
    "\n",
    "# Print translated subsets to file\n",
    "filename = 'bart_classifications.tsv' \n",
    "classified_data.to_csv(filename, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
