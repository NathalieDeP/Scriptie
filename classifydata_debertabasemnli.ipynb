{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base-mnli')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-base-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gold label and pairIDs\n",
    "labels = ['entailment', 'contradiction', 'neutral']\n",
    "gold_labels = pd.read_csv('subset_en.csv', sep='\\t')\n",
    "classified_data = pd.DataFrame(columns=['gold_label', 'English', 'German', 'Spanish', 'Swahili', 'Urdu', 'pairID'])\n",
    "classified_data['gold_label'] = gold_labels['gold_label']\n",
    "classified_data['pairID'] = gold_labels['pairID']\n",
    "\n",
    "# Classify English\n",
    "data = gold_labels\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in data.iterrows():\n",
    "        # Get needed info\n",
    "        sentence1 = row['sentence1']\n",
    "        sentence2 = row['sentence2']\n",
    "\n",
    "        # Classify\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=200,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        # Pass the encoded input through the model to obtain the predicted label\n",
    "        outputs = model(**encoded_input)\n",
    "        predicted_logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(predicted_logits, dim=1)\n",
    "\n",
    "        # Add label to classified_data\n",
    "        classified_data.at[index, 'English'] = labels[predicted_labels.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify German to English\n",
    "data = pd.read_csv('translated_subset_de.tsv', sep='\\t')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in data.iterrows():\n",
    "        # Get needed info\n",
    "        sentence1 = row['translated_sentence1']\n",
    "        sentence2 = row['translated_sentence2']\n",
    "\n",
    "        # Classify\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=200,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        # Pass the encoded input through the model to obtain the predicted label\n",
    "        outputs = model(**encoded_input)\n",
    "        predicted_logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(predicted_logits, dim=1)\n",
    "\n",
    "        # Add label to classified_data\n",
    "        classified_data.at[index, 'German'] = labels[predicted_labels.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify Spanish to English\n",
    "data = pd.read_csv('translated_subset_es.tsv', sep='\\t')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in data.iterrows():\n",
    "        # Get needed info\n",
    "        sentence1 = row['translated_sentence1']\n",
    "        sentence2 = row['translated_sentence2']\n",
    "\n",
    "        # Classify\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=200,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        # Pass the encoded input through the model to obtain the predicted label\n",
    "        outputs = model(**encoded_input)\n",
    "        predicted_logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(predicted_logits, dim=1)\n",
    "\n",
    "        # Add label to classified_data\n",
    "        classified_data.at[index, 'Spanish'] = labels[predicted_labels.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify Swahili to English\n",
    "data = pd.read_csv('translated_subset_sw.tsv', sep='\\t')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in data.iterrows():\n",
    "        # Get needed info\n",
    "        sentence1 = row['translated_sentence1']\n",
    "        sentence2 = row['translated_sentence2']\n",
    "\n",
    "        # Classify\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=200,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        # Pass the encoded input through the model to obtain the predicted label\n",
    "        outputs = model(**encoded_input)\n",
    "        predicted_logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(predicted_logits, dim=1)\n",
    "\n",
    "        # Add label to classified_data\n",
    "        classified_data.at[index, 'Swahili'] = labels[predicted_labels.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify Urdu to English\n",
    "data = pd.read_csv('translated_subset_ur.tsv', sep='\\t')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in data.iterrows():\n",
    "        # Get needed info\n",
    "        sentence1 = row['translated_sentence1']\n",
    "        sentence2 = row['translated_sentence2']\n",
    "\n",
    "        # Classify\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=200,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        # Pass the encoded input through the model to obtain the predicted label\n",
    "        outputs = model(**encoded_input)\n",
    "        predicted_logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(predicted_logits, dim=1)\n",
    "\n",
    "        # Add label to classified_data\n",
    "        classified_data.at[index, 'Urdu'] = labels[predicted_labels.item()]\n",
    "\n",
    "# Print translated subsets to file\n",
    "filename = 'deberta_classifications.tsv' \n",
    "classified_data.to_csv(filename, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
