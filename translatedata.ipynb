{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\depal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\depal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\depal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "\n",
    "import transformers\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate German to English\n",
    "data = pd.read_csv('subset_de.csv', sep='\\t')\n",
    "tokenizer.src_lang = \"de\"\n",
    "\n",
    "translated_data = pd.DataFrame(columns=['translated_sentence1', 'translated_sentence2', 'pairID'])\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    #Get needed info\n",
    "    sentence1 = row['sentence1']\n",
    "    sentence2 = row['sentence2']\n",
    "    pairid = row['pairID']\n",
    "\n",
    "    # Translate first sentence\n",
    "    encoded_sentence1 = tokenizer.encode_plus(sentence1, return_tensors=\"pt\", padding=\"longest\")\n",
    "    generated_tokens1 = model.generate(\n",
    "        input_ids=encoded_sentence1['input_ids'],\n",
    "        attention_mask=encoded_sentence1['attention_mask'],\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "    decoded_text1 = tokenizer.decode(generated_tokens1[0], skip_special_tokens=True)\n",
    "\n",
    "    # Translate second sentence\n",
    "    encoded_sentence2 = tokenizer.encode_plus(sentence2, return_tensors=\"pt\", padding=\"longest\")\n",
    "    generated_tokens2 = model.generate(\n",
    "        input_ids=encoded_sentence2['input_ids'],\n",
    "        attention_mask=encoded_sentence2['attention_mask'],\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "    decoded_text2 = tokenizer.decode(generated_tokens2[0], skip_special_tokens=True)\n",
    "\n",
    "    # Make new dataframe for translated sentences\n",
    "    translated_data.loc[index] = [decoded_text1, decoded_text2, pairid]\n",
    "\n",
    "\n",
    "# Print translated subsets to file\n",
    "filename = 'translated_subset_de.tsv' \n",
    "translated_data.to_csv(filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate Spanish to English\n",
    "data = pd.read_csv('subset_es.csv', sep='\\t')\n",
    "tokenizer.src_lang = \"es\"\n",
    "\n",
    "translated_data = pd.DataFrame(columns=['translated_sentence1', 'translated_sentence2', 'pairID'])\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    #Get needed info\n",
    "    sentence1 = row['sentence1']\n",
    "    sentence2 = row['sentence2']\n",
    "    pairid = row['pairID']\n",
    "\n",
    "    # Translate first sentence\n",
    "    encoded_sentence1 = tokenizer.encode_plus(sentence1, return_tensors=\"pt\", padding=\"longest\")\n",
    "    generated_tokens1 = model.generate(\n",
    "        input_ids=encoded_sentence1['input_ids'],\n",
    "        attention_mask=encoded_sentence1['attention_mask'],\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "    decoded_text1 = tokenizer.decode(generated_tokens1[0], skip_special_tokens=True)\n",
    "\n",
    "    # Translate second sentence\n",
    "    encoded_sentence2 = tokenizer.encode_plus(sentence2, return_tensors=\"pt\", padding=\"longest\")\n",
    "    generated_tokens2 = model.generate(\n",
    "        input_ids=encoded_sentence2['input_ids'],\n",
    "        attention_mask=encoded_sentence2['attention_mask'],\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "    decoded_text2 = tokenizer.decode(generated_tokens2[0], skip_special_tokens=True)\n",
    "\n",
    "    # Make new dataframe for translated sentences\n",
    "    translated_data.loc[index] = [decoded_text1, decoded_text2, pairid]\n",
    "\n",
    "\n",
    "# Print translated subsets to file\n",
    "filename = 'translated_subset_es.tsv' \n",
    "translated_data.to_csv(filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate Swahili to English\n",
    "data = pd.read_csv('subset_sw.csv', sep='\\t')\n",
    "tokenizer.src_lang = \"sw\"\n",
    "\n",
    "translated_data = pd.DataFrame(columns=['translated_sentence1', 'translated_sentence2', 'pairID'])\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    #Get needed info\n",
    "    sentence1 = row['sentence1']\n",
    "    sentence2 = row['sentence2']\n",
    "    pairid = row['pairID']\n",
    "\n",
    "    # Translate first sentence\n",
    "    encoded_sentence1 = tokenizer.encode_plus(sentence1, return_tensors=\"pt\", padding=\"longest\")\n",
    "    generated_tokens1 = model.generate(\n",
    "        input_ids=encoded_sentence1['input_ids'],\n",
    "        attention_mask=encoded_sentence1['attention_mask'],\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "    decoded_text1 = tokenizer.decode(generated_tokens1[0], skip_special_tokens=True)\n",
    "\n",
    "    # Translate second sentence\n",
    "    encoded_sentence2 = tokenizer.encode_plus(sentence2, return_tensors=\"pt\", padding=\"longest\")\n",
    "    generated_tokens2 = model.generate(\n",
    "        input_ids=encoded_sentence2['input_ids'],\n",
    "        attention_mask=encoded_sentence2['attention_mask'],\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "    decoded_text2 = tokenizer.decode(generated_tokens2[0], skip_special_tokens=True)\n",
    "\n",
    "    # Make new dataframe for translated sentences\n",
    "    translated_data.loc[index] = [decoded_text1, decoded_text2, pairid]\n",
    "\n",
    "\n",
    "# Print translated subsets to file\n",
    "filename = 'translated_subset_sw.tsv' \n",
    "translated_data.to_csv(filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate Urdu to English\n",
    "data = pd.read_csv('subset_ur.csv', sep='\\t')\n",
    "tokenizer.src_lang = \"ur\"\n",
    "\n",
    "translated_data = pd.DataFrame(columns=['translated_sentence1', 'translated_sentence2', 'pairID'])\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    #Get needed info\n",
    "    sentence1 = row['sentence1']\n",
    "    sentence2 = row['sentence2']\n",
    "    pairid = row['pairID']\n",
    "\n",
    "    # Translate first sentence\n",
    "    encoded_sentence1 = tokenizer.encode_plus(sentence1, return_tensors=\"pt\", padding=\"longest\")\n",
    "    generated_tokens1 = model.generate(\n",
    "        input_ids=encoded_sentence1['input_ids'],\n",
    "        attention_mask=encoded_sentence1['attention_mask'],\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "    decoded_text1 = tokenizer.decode(generated_tokens1[0], skip_special_tokens=True)\n",
    "\n",
    "    # Translate second sentence\n",
    "    encoded_sentence2 = tokenizer.encode_plus(sentence2, return_tensors=\"pt\", padding=\"longest\")\n",
    "    generated_tokens2 = model.generate(\n",
    "        input_ids=encoded_sentence2['input_ids'],\n",
    "        attention_mask=encoded_sentence2['attention_mask'],\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(\"en\")\n",
    "    )\n",
    "    decoded_text2 = tokenizer.decode(generated_tokens2[0], skip_special_tokens=True)\n",
    "\n",
    "    # Make new dataframe for translated sentences\n",
    "    translated_data.loc[index] = [decoded_text1, decoded_text2, pairid]\n",
    "\n",
    "\n",
    "# Print translated subsets to file\n",
    "filename = 'translated_subset_ur.tsv' \n",
    "translated_data.to_csv(filename, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
